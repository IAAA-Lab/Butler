#Docker container Operating System
dockerOS:
  name: ubuntu
  version: 10.0

#Crawling System
crawlSystem:
    name: nutch
    version: 1.9
    #seeds are separated by line break and preceded by '- '
    seeds:
      - https://dzone.com/articles/using-yaml-java-application
      - https://dzone.com/articles/
    #number of rounds the crawler is going to iterate
    rounds: 2
    extraction: round
    #seeds are separated by line break and preceded by '- '
        #plugins are separated by line break and preceded by '- '
    #Plugins are not specified in the DSL, they have to be in a folder named 'Plugins' in the same level of the
    #DSL file. This folder must have one folder for every plugin you want to add and that folder must have the
    #compiled plugin ( plugin.xml and its jar files)
    
    #the structure of the crawled info, you can choose between text or hmtl
    infoCrawled: text
    #The length limit for downloaded content
    maxFileLength: 65536 # file.content.limit  and http.content.limit
    # If the Crawl-Delay in robots.txt is set to greater than this value (in seconds)
    # then the fetcher will skip this page, generating an error report. If set to -1
    # the fetcher will never skip such pages and will wait the amount of time retrieved from robots.txt Crawl-Delay, however long that might be.
    maxCrawlDelay: 50 # fetcher.max.crawl.delay
    #Maximum number of Inlinks per URL to be kept in LinkDb.
    linksLimitURL: 10000 # db.max.inlinks
    # If true, when adding new links to a page, links from the same host are ignored.
    # Determines how to put URLs into queues. Default value is 'byHost', also takes 'byDomain' or 'byIP'.
    queueMode: byHost # fetcher.queue.mode
    timeouts:
      #	Timeout in seconds for the parsing of a document
      parser: 30 # parser.timeout
      #The number of times a thread will delay when trying to fetch a page
      fetchTimes: 100 # http.timeout
      #The default network timeout, in millisecond
      network: 10000 # http.timeout

