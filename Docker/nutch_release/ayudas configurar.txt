Ayudas para configurar:
para el plugin mirar la carpeta de plugins y lo del nutch.site
- Poder aÃ±adir plugins ->
In order to get Nutch to use your plugin, you need to edit your conf/nutch-site.xml file and add in a block like this:
<property>
  <name>plugin.includes</name>
  <value>protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|scoring-opic|urlnormalizer-(pass|regex|basic)|urlmeta</value>
  <description>Regular expression naming plugin directory names to
  include.  Any plugin not matching this expression is excluded.
  In any case you need at least include the nutch-extensionpoints plugin. By
  default Nutch includes crawling just HTML and plain text via HTTP,
  and basic indexing and search plugins.
  </description>
</property>
hay que tocar algo de esto??

plugin.includes	protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)	Regular expression naming plugin directory names to include. Any plugin not matching this expression is excluded. In any case you need at least include the nutch-extensionpoints plugin. By default Nutch includes crawling just HTML and plain text via HTTP, and basic indexing and search plugins. In order to use HTTPS please enable protocol-httpclient, but be aware of possible intermittent problems with the underlying commons-httpclient library.


#the structure of the crawled info, you can choose between text or hmtl
infoCrawled: text
Que por defecto sea -nogenerate -nofetch -noparse -nocontent -noparsedata
maxFileLength: 65536 # file.content.limit  and http.content.limit
# If the Crawl-Delay in robots.txt is set to greater than this value (in seconds)
# then the fetcher will skip this page, generating an error report. If set to -1
# the fetcher will never skip such pages and will wait the amount of time retrieved from robots.txt Crawl-Delay, however long that might be.
maxCrawlDelay: 50 # fetcher.max.crawl.delay
#Maximum number of Inlinks per URL to be kept in LinkDb.
linksLimitURL: 10000 # db.max.inlinks
# If true, when adding new links to a page, links from the same host are ignored.
queueMode: byHost # fetcher.queue.mode
Timeouts:
  #	Timeout in seconds for the parsing of a document
  parser: 30 # parser.timeout
  #The number of times a thread will delay when trying to fetch a page
  fetchTimes: 100 # http.timeout
  #The default network timeout, in millisecond
  network: 10000 # http.timeout
