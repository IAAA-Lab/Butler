#Container Operating System
OSbase:
  name: ubuntu
  version: 14.04

#Crawling System
crawlSystem:
    name: nutch
    version: 1.9
    #seeds are separated by spaces
    seeds: https://dzone.com/articles/using-yaml-java-application https://dzone.com/articles/
    #number of rounds the crawler is going to iterate
    rounds: 2
    #plugins are separated by spaces
    plugins: nombrePlugin1 nombrePlugin2
    #the structure of the crawled info, you can choose between text or hmtl
    infoCrawled: text
    #The length limit for downloaded content
    maxFileLength: 65536 # file.content.limit  and http.content.limit
    # If the Crawl-Delay in robots.txt is set to greater than this value (in seconds)
    # then the fetcher will skip this page, generating an error report. If set to -1
    # the fetcher will never skip such pages and will wait the amount of time retrieved from robots.txt Crawl-Delay, however long that might be.
    maxCrawlDelay: 50 # fetcher.max.crawl.delay
    #Maximum number of Inlinks per URL to be kept in LinkDb.
    linksLimitURL: 10000 # db.max.inlinks
    # If true, when adding new links to a page, links from the same host are ignored.
    internalLinks: true # db.ignore.internal.links
    # Determines how to put URLs into queues. Default value is 'byHost', also takes 'byDomain' or 'byIP'.
    queueMode: byHost # fetcher.queue.mode
    Timeouts:
      #	Timeout in seconds for the parsing of a document
      parser: 30 # parser.timeout
      #The number of times a thread will delay when trying to fetch a page
      fetchTimes: 100 # http.timeout
      #The default network timeout, in millisecond
      network: 10000 # http.timeout
